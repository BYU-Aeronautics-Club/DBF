---
layout: post
title: "2022 Competition Archive"
description: "Proposal, Report, Feedback, etc."
author: "Judd Mehr"
---

# Overview


## Team Makeup

During the first semester of the competition we had a total of 15 members, composed of 9 underclassmen, 4 seniors, and 2 graduate students.

## Competition Rules

For reference, the rules for the competition can be downloaded <a href="{{ site.baseurl }}/assets/dbf-rules-2022.pdf" download>here</a>.  Note that the draft rules used a shock sensor sensitivity of 5g, while the official rules increased the acceptable shock to 25g.

# Competition Proposal

Our submitted proposal can be downloaded <a href="{{ site.baseurl }}/assets/2022DBF_Brigham_Young_University_Provo_PROPOSAL.pdf" download>here</a>.  We received an overall score of 72.75 (top score was 85), placing us <a href="{{ site.baseurl }}/assets/2022_DBF_Proposal_Scores.pdf" download>43rd of 110</a> selected teams.

## Feedback from Judges

Below are the directly quoted feedback points received from the competition judges on our proposal.

1. Largest reductions was in the schedule section. They were hard to read and dates were not provided. Team organization in both Management and Exec Summary were vague, resulting in points. Last 3 sections appeared to address every issue with some detail.
1. Very strong objective statement, but lacks elaboration of deployment mechanism. Excellent description of roles and responsibilities, but lacks detail of decision-making process. The meaning of the arrows in the org chart is unclear. The schedule is straightforward and concise but does omit subcomponent design. The budget is clear and complete, but the table title is too long. The preliminary design is exceptional. Vial shock during different flight phases is mentioned, but no analysis is provided. Lacks detail of release mechanism. The initial sizing is excellent and the sensitivity study is reasonable and clear. Manufacturing flow description is strong. Only one critical process is discussed, but that discussion is good. Schedule is reasonable and concise. Subsystem component design discussion is strong. Specific testing data collection and its design implications are lacking. Omits specific schedules for ground and flight testing. Flight test plan is discussed, but few specifics provided.

## Takeaways for next year

We did a pretty good job of addressing all of the feedback from last year.  The only places where feedback overlapped somewhat seemed to be in our testing plan, where we have the least experience.  Progress has been made, but we still have lots of room for improvement.  Some specific items that we'll want to consider for next year's proposal include:

1. The schedule gantt chart needs to be bigger next year for easy of reading. We can't assume that the judges are reading the document on a monitor as big as ours, or even that they are able to zoom it at all.
1. Along with being bigger, we should include more details about specific sub-systems in the gantt chart rather than just a general schedule.
1. We need to reference specific dates in the text portion of the schedule section, probably down to the actual day. These can simply be the major milestones we set for ourselves.
1. Team *organization* descriptions can be improved (the individual job descriptions are fine), specifically in addressing the decision making structure and explicitly stating who reports to whom. (basically explaining the arrows in the organization figure)
1. We need to do better at getting details sooner for the misc. systems (usually to do with the payload etc.) and getting those details in the proposal.  There were about 3 comments about our payload system this year lacking detail.
1. We probably should have done some official testing on the shock sensors and included data for that to support our design decsions. And in future years, we should include details about any tests/analyses we've used in our conceptual design/decision making process.
1. Shorten the table title (“Budget,” is probably short enough…)
1. Sensitivity study is okay, but could probably be better.  In fact, it would be best to have a much more in depth mission analysis code at some point.
1. We can spend a bit more time on more than one critical process. (We spent a lot of words on composites this year since it really was the number 1 critical process this year)
1. We need to do a better job at designing specific tests from whose results we obtain data for making design decisions, then explaining all that in the proposal.
1. We also need to add a testing schedule, or include specific tests in our expanded schedule mentioned above.
1. We need to add more details for flight test plans, and probably all our testing plans.
1. Overall: we need more details. Next year, take everything that is in any way vague, and quantify it.

# Final Design Report

## Feedback from Judges

## Takeaways for next year